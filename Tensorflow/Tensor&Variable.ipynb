{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungbae/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "1. Graph and Session\n",
    "    - TensorFlow Program: Build a Computation Graph (tf.Graph)  \n",
    "    - TensorFlow Runtime: Run the Graph using a Session (tf.Session)\n",
    "2. Training \n",
    "    - Graph: \n",
    "        - Reset Graph $\\rightarrow$ Define a Pipeline $\\rightarrow$ Define a Model $\\rightarrow$ Define a Loss $\\rightarrow$ Define a Optimizer $\\rightarrow$ Operation for init Variables\n",
    "    - Session: \n",
    "        - Creating a Session $\\rightarrow$ Feeding Data $\\rightarrow$ Updating Variables $\\rightarrow$ Release Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for Recap\n",
    "\n",
    "Let's recap the basic of TensorFlow through a model represnting the equation **$y = 2x + 1$**.\n",
    "\n",
    "### TensorFlow Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X: [ 0  5 10 15 20]\n",
      "Train Y: [ 1 11 21 31 41]\n",
      "Test X: [     1      2      3     10     14 135799  25787]\n",
      "Test Y: [     3      5      7     21     29 271599  51575]\n",
      "\n",
      "Shape of X: (200, 1) \n",
      "Shape of Y: (200, 1) \n",
      "Shape of Test: (7, 1)\n"
     ]
    }
   ],
   "source": [
    "## Values for Model\n",
    "data_x = np.arange(0, 1000, 5).reshape(-1, 1)\n",
    "data_y = 2 * np.arange(0, 1000, 5).reshape(-1, 1) + 1\n",
    "test_x = np.array([1,2,3,10,14,135799, 25787]).reshape(-1, 1)\n",
    "test_y = 2 * test_x + 1\n",
    "\n",
    "print('Train X:', data_x[:5].flatten())\n",
    "print('Train Y:', data_y[:5].flatten())\n",
    "print('Test X:', test_x.flatten())\n",
    "print('Test Y:', test_y.flatten())\n",
    "\n",
    "## Normalization\n",
    "data_x = data_x / 999\n",
    "data_y = (data_y - 1) / 1999\n",
    "test_x = test_x / 999\n",
    "\n",
    "print('\\nShape of X:', data_x.shape, \n",
    "      '\\nShape of Y:', data_y.shape, \n",
    "      '\\nShape of Test:', test_x.shape)\n",
    "\n",
    "# 1. Reset Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 2. Define a Pipeline\n",
    "## Placeholders\n",
    "inputs = tf.placeholder(tf.float32, shape=(None, 1), name='input')\n",
    "y_true = tf.placeholder(tf.float32, shape=(None, 1), name='label')\n",
    "\n",
    "# 3. Define a Model\n",
    "with tf.variable_scope('LinearModel') as vs:\n",
    "    hidden = tf.layers.Dense(units=100)(inputs)\n",
    "    y_pred = tf.layers.Dense(units=1)(hidden)\n",
    "\n",
    "# 4. Define a Loss\n",
    "loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)\n",
    "\n",
    "# 5. Define a Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 6. Operation for Initialization of Variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# TensorBoard\n",
    "writer = tf.summary.FileWriter('./recap')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36977234\n",
      "0 | Model Result: [     6      6      5      5      4 -14709  -2788]\n",
      "0 | Truth: [     3      5      7     21     29 271599  51575]\n",
      "\n",
      "3.56648e-06\n",
      "10000 | Model Result: [    10     12     13     27     35 269854  51249]\n",
      "10000 | Truth: [     3      5      7     21     29 271599  51575]\n",
      "\n",
      "9.969338e-10\n",
      "20000 | Model Result: [     3      5      7     21     29 271569  51569]\n",
      "20000 | Truth: [     3      5      7     21     29 271599  51575]\n",
      "\n",
      "1.4406375e-10\n",
      "30000 | Model Result: [     3      5      7     21     29 271587  51572]\n",
      "30000 | Truth: [     3      5      7     21     29 271599  51575]\n",
      "\n",
      "7.3755e-11\n",
      "40000 | Model Result: [     3      5      7     21     29 271590  51573]\n",
      "40000 | Truth: [     3      5      7     21     29 271599  51575]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Define a Session\n",
    "sess = tf.Session()\n",
    "\n",
    "# 8. Init Variables\n",
    "sess.run(init)\n",
    "\n",
    "# 9. Feeding a Data\n",
    "# 10. Updating Variables\n",
    "for i in range(50000):\n",
    "    _, loss_value = sess.run([train, loss], \n",
    "                             {inputs: data_x, y_true: data_y})\n",
    "    if i % 10000 == 0:\n",
    "        print(loss_value)\n",
    "        test_result = sess.run(y_pred, feed_dict={inputs: test_x})\n",
    "        print(f'{i} | Model Result:', (test_result * 1999 + 1).flatten().astype(int))\n",
    "        print(f'{i} | Truth:', test_y.flatten())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Result: [     3      5      7     21     29 271592  51573]\n",
      "Truth: [     3      5      7     21     29 271599  51575]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_result = sess.run(y_pred, feed_dict={inputs: test_x})\n",
    "\n",
    "print('Model Result:', (test_result * 1999 + 1).flatten().astype(int))\n",
    "print('Truth:', test_y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Release Resources\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "1. Operation에 전달되는 값을 다루기 위해, Tensor 객체에 알아본다.\n",
    "    - Tensor의 **모양을 확인**하는 방법, Tensor에서 **원하는 값을 선택**하는 방법, Tensor의 **데이터 타입을 변경**하는 방법.\n",
    "2. 모델을 구성하는 파라미터를 다루기 위해, Variable을 알아본다.  \n",
    "    - Variable에 **접근**하는 방법, Variable에 **값을 할당**하는 방법, Variable을 **재활용**하는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Tensor\n",
    "\n",
    "- 값을 생성하기로 약속된 객체(원문 참조).\n",
    "- 동일 데이터 타입을 담은 고차원 array로 표현. \n",
    "- 연산에 사용되는 값을 담은 객체이기 때문에, shape과 dtype이 주요 관심사.  \n",
    "- 대부분의 operation들은 모양(shape)이 완전히 알려진 tensor를 출력.\n",
    "- 하지만, 일부는 모양이 일부분만 알려진 tensor를 출력하며, 이때는 graph 실행시 모양을 알 수 있다.\n",
    "\n",
    "> 원문: A `tf.Tensor` object represents a **partially defined computation that will eventually produce a value.**\n",
    "\n",
    "## A.1) Rank\n",
    "Tensor의 차원을 나타낸다.\n",
    "\n",
    "<!-- > Note: Matrix의 rank와는 같지 않다. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Rank | Math entity|\n",
    "|-----|------------|\n",
    "|0|\tScalar (magnitude only)|\n",
    "|1|\tVector (magnitude and direction)|\n",
    "|2|\tMatrix (table of numbers)|\n",
    "|3|\t3-Tensor (cube of numbers)|\n",
    "|n|\tn-Tensor (you get the idea)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.a) Rank 0\n",
    "\n",
    "생성할 값만 입력하면 rank 0인 tensor가 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n",
      "Tensor(\"Const_1:0\", shape=(), dtype=int16)\n",
      "Tensor(\"Const_2:0\", shape=(), dtype=float64)\n",
      "Tensor(\"Const_3:0\", shape=(), dtype=complex64)\n",
      "[b'Elephant', 451, 3.14159265359, (12.3-4.85j)]\n"
     ]
    }
   ],
   "source": [
    "# Reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Tensors\n",
    "mammal = tf.constant(\"Elephant\", tf.string)\n",
    "ignition = tf.constant(451, tf.int16)\n",
    "floating = tf.constant(3.14159265359, tf.float64)\n",
    "its_complicated = tf.constant(12.3 - 4.85j, tf.complex64)\n",
    "\n",
    "# Outputs of operations\n",
    "print(mammal, ignition, floating, its_complicated, sep='\\n')\n",
    "\n",
    "# Tensor Values\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([mammal, ignition, floating, its_complicated]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.b) Rank 1\n",
    "\n",
    "`list`로 값을 입력하면 rank 1인 tensor가 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(1,), dtype=string)\n",
      "Tensor(\"Const_1:0\", shape=(2,), dtype=float32)\n",
      "Tensor(\"Const_2:0\", shape=(5,), dtype=int32)\n",
      "Tensor(\"Const_3:0\", shape=(2,), dtype=complex64)\n",
      "[array([b'Hello'], dtype=object), array([3.14159, 2.71828], dtype=float32), array([ 2,  3,  5,  7, 11], dtype=int32), array([12.3-4.85j,  7.5-6.23j], dtype=complex64)]\n"
     ]
    }
   ],
   "source": [
    "# Reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Tensors\n",
    "mystr = tf.constant([\"Hello\"], tf.string)\n",
    "cool_numbers  = tf.constant([3.14159, 2.71828], tf.float32)\n",
    "first_primes = tf.constant([2, 3, 5, 7, 11], tf.int32)\n",
    "its_very_complicated = tf.constant([12.3 - 4.85j, 7.5 - 6.23j], tf.complex64)\n",
    "\n",
    "print(mystr, cool_numbers, first_primes, its_very_complicated, sep='\\n')\n",
    "\n",
    "# Run Graph\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([mystr, cool_numbers, first_primes, its_very_complicated]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.c) Higher Ranks\n",
    "\n",
    "row와 column이 존재하는 list를 입력하면 rank 2이상의 tensor가 생성.  \n",
    "Rank는 그래프 실행시 `tf.rank`로 확인 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(2, 1), dtype=int16)\n",
      "Tensor(\"Const_1:0\", shape=(2, 2), dtype=bool)\n",
      "Tensor(\"Const_2:0\", shape=(4, 1), dtype=int32)\n",
      "Tensor(\"Const_3:0\", shape=(2, 2), dtype=int32)\n",
      "Tensor(\"Const_4:0\", shape=(2, 1), dtype=int32)\n",
      "Tensor(\"Rank:0\", shape=(), dtype=int32)\n",
      "[array([[ 7],\n",
      "       [11]], dtype=int16), array([[False,  True],\n",
      "       [ True, False]]), array([[ 4],\n",
      "       [ 9],\n",
      "       [16],\n",
      "       [25]], dtype=int32), array([[ 4,  9],\n",
      "       [16, 25]], dtype=int32), array([[ 7],\n",
      "       [11]], dtype=int32)]\n",
      "Rank of squares: 2\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Tensors\n",
    "mymat = tf.constant([[7],[11]], tf.int16)\n",
    "myxor = tf.constant([[False, True],[True, False]], tf.bool)\n",
    "linear_squares = tf.constant([[4], [9], [16], [25]], tf.int32)\n",
    "squarish_squares = tf.constant([ [4, 9], [16, 25] ], tf.int32)\n",
    "mymatC = tf.constant([[7],[11]], tf.int32)\n",
    "\n",
    "rank_of_squares = tf.rank(squarish_squares)\n",
    "\n",
    "\n",
    "print(mymat, myxor, linear_squares, squarish_squares, mymatC, sep='\\n')\n",
    "print(rank_of_squares)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([mymat, myxor, linear_squares, squarish_squares, mymatC]))\n",
    "    print('Rank of squares:', sess.run(rank_of_squares))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2) Slice를 통한 tensor값 참조\n",
    "\n",
    "slice를 통해 tensor의 일부분을 참조할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"Const_1:0\", shape=(3, 3), dtype=int32)\n",
      "Tensor(\"strided_slice:0\", shape=(), dtype=int32)\n",
      "Tensor(\"strided_slice_1:0\", shape=(), dtype=int32)\n",
      "Scalar: [1, 6]\n",
      "Vector: [array([7, 8, 9], dtype=int32), array([3, 6, 9], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "my_vector = tf.constant([1,2,3])\n",
    "my_matrix = tf.constant([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(my_vector, my_matrix, sep='\\n')\n",
    "\n",
    "# Slicing\n",
    "my_scalar1 = my_vector[0]\n",
    "my_scalar2 = my_matrix[1, 2]\n",
    "print(my_scalar1, my_scalar2, sep='\\n')\n",
    "\n",
    "my_row_vector = my_matrix[2]\n",
    "my_column_vector = my_matrix[:, 2]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('Scalar:', sess.run([my_scalar1, my_scalar2]))\n",
    "    print('Vector:', sess.run([my_row_vector, my_column_vector]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3) Shape\n",
    "Tensor의 각 차원내 속한 값들의 갯수. Graph 생성시 자동으로 추측한다.  \n",
    "These inferred shapes might have known or unknown rank. If the rank is known, the sizes of each dimension might be known or unknown.\n",
    "\n",
    "The TensorFlow documentation uses three notational conventions to describe tensor dimensionality: rank, shape, and dimension number. The following table shows how these relate to one another:\n",
    "\n",
    "\n",
    "|Rank | Shape | Dimension number | Example |\n",
    "| ----------------------|\n",
    "|0|[]|0-D|A 0-D tensor. A scalar.|\n",
    "|1|[D0]|1-D|A 1-D tensor with shape [5].|\n",
    "|2|[D0, D1]|2-D|A 2-D tensor with shape [3, 4].|\n",
    "|3|[D0, D1, D2]|3-D|A 3-D tensor with shape [1, 4, 3].|\n",
    "|n|[D0, D1, ... Dn-1]|n-D|A tensor with shape [D0, D1, ... Dn-1].|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3.a) Tensor의 shape 획득\n",
    "\n",
    "Tensor의 shape 획득에는 2가지 방법이 존재한다.\n",
    "1. Graph 생성시 shape 획득\n",
    "2. Graph 실행시 shape 획득\n",
    "\n",
    "#### A.3.a-1) Graph 생성시 shape 획득\n",
    "\n",
    "이미 모양을 알고 있는 tensor의 shape획득에 유용하다. `tf.Tensor`객체의 shape 속성을 읽어, `TensorShape`객체로 반환한다.   \n",
    "\n",
    "#### A.3.a-2) Graph 실행시 shape 획득\n",
    "\n",
    "특정 `tf.Tensor`의 shape을 획득할 수 있는 `tf.shape` operation으로 획득. \n",
    "해당 operation은 `tf.Tensor`을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1:\n",
      "Type: <class 'tensorflow.python.framework.tensor_shape.TensorShape'> / Shape: (3,)\n",
      "Type: <class 'tensorflow.python.framework.tensor_shape.TensorShape'> / Shape: (3, 3)\n",
      "\n",
      "Case 2:\n",
      "Type: <class 'tensorflow.python.framework.ops.Tensor'> / Shape: [3]\n",
      "Type: <class 'tensorflow.python.framework.ops.Tensor'> / Shape: [3 3]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "my_vector = tf.constant([1,2,3])\n",
    "my_matrix = tf.constant([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "print('Case 1:')\n",
    "print('Type:', type(my_vector.shape), '/ Shape:', my_vector.shape)\n",
    "print('Type:', type(my_matrix.shape), '/ Shape:', my_matrix.shape)\n",
    "\n",
    "shape_vec = tf.shape(my_vector)\n",
    "shape_mat = tf.shape(my_matrix)\n",
    "\n",
    "writer = tf.summary.FileWriter('./shape')\n",
    "writer.add_graph(tf.get_default_graph())\n",
    "writer.flush()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('\\nCase 2:')\n",
    "    print('Type:', type(shape_vec), '/ Shape:', sess.run(shape_vec))\n",
    "    print('Type:', type(shape_mat), '/ Shape:', sess.run(shape_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3.b) Tensor의 shape 변경\n",
    "\n",
    "`tf.reshape`을 통해서 변경이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones:0\", shape=(3, 4, 5), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(6, 10), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(3, 20), dtype=float32)\n",
      "Tensor(\"Reshape_2:0\", shape=(4, 3, 5), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension size must be evenly divisible by 26 but is 60 for 'Reshape_3' (op: 'Reshape') with input shapes: [4,3,5], [3] and with input tensors computed as partial shapes: input[1] = [13,2,?].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension size must be evenly divisible by 26 but is 60 for 'Reshape_3' (op: 'Reshape') with input shapes: [4,3,5], [3] and with input tensors computed as partial shapes: input[1] = [13,2,?].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-797a321d69a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# error because no possible value for the last dimension will match the number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# of elements.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0myet_another\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrixAlt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ERROR!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6480\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6481\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6482\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6483\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6484\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3274\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1792\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension size must be evenly divisible by 26 but is 60 for 'Reshape_3' (op: 'Reshape') with input shapes: [4,3,5], [3] and with input tensors computed as partial shapes: input[1] = [13,2,?]."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "rank_three_tensor = tf.ones([3, 4, 5])\n",
    "print(rank_three_tensor)\n",
    "\n",
    "matrix = tf.reshape(rank_three_tensor, [6, 10])  # Reshape existing content into\n",
    "                                                 # a 6x10 matrix\n",
    "print(matrix)\n",
    "\n",
    "matrixB = tf.reshape(matrix, [3, -1])  #  Reshape existing content into a 3x20\n",
    "                                       # matrix. -1 tells reshape to calculate\n",
    "                                       # the size of this dimension.\n",
    "print(matrixB)\n",
    "        \n",
    "matrixAlt = tf.reshape(matrixB, [4, 3, -1])  # Reshape existing content into a\n",
    "                                             #4x3x5 tensor\n",
    "print(matrixAlt)\n",
    "\n",
    "# Note that the number of elements of the reshaped Tensors has to match the\n",
    "# original number of elements. Therefore, the following example generates an\n",
    "# error because no possible value for the last dimension will match the number\n",
    "# of elements.\n",
    "yet_another = tf.reshape(matrixAlt, [13, 2, -1])  # ERROR!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4) Tensor의 데이터타입 변경\n",
    "\n",
    "`tf.cast`를 이용하여, 데이터타입이 변경된 tensor의 획득이 가능하다.\n",
    "\n",
    "#### 데이터 타입\n",
    "1. `tf.float16`: 16-bit half-precision floating-point.\n",
    "2. `tf.float32`: 32-bit single-precision floating-point.\n",
    "3. `tf.float64`: 64-bit double-precision floating-point.\n",
    "4. `tf.bfloat16`: 16-bit truncated floating-point.\n",
    "3. `tf.complex64`: 64-bit single-precision complex.\n",
    "3. `tf.complex128`: 128-bit double-precision complex.\n",
    "3. `tf.int8`: 8-bit signed integer.\n",
    "3. `tf.uint8`: 8-bit unsigned integer.\n",
    "3. `tf.uint16`: 16-bit unsigned integer.\n",
    "3. `tf.uint32`: 32-bit unsigned integer.\n",
    "3. `tf.uint64`: 64-bit unsigned integer.\n",
    "3. `tf.int16`: 16-bit signed integer.\n",
    "3. `tf.int32`: 32-bit signed integer.\n",
    "3. `tf.int64`: 64-bit signed integer.\n",
    "3. `tf.bool`: Boolean.\n",
    "3. `tf.string`: String.\n",
    "3. `tf.qint8`: Quantized 8-bit signed integer.\n",
    "3. `tf.quint8`: Quantized 8-bit unsigned integer.\n",
    "3. `tf.qint16`: Quantized 16-bit signed integer.\n",
    "3. `tf.quint16`: Quantized 16-bit unsigned integer.\n",
    "3. `tf.qint32`: Quantized 32-bit signed integer.\n",
    "3. `tf.resource`: Handle to a mutable resource.\n",
    "3. `tf.variant`: Values of arbitrary types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(4,), dtype=int32)\n",
      "Tensor(\"Const:0\", shape=(4,), dtype=int32) Tensor(\"Cast:0\", shape=(4,), dtype=float32) Tensor(\"Cast_1:0\", shape=(4,), dtype=bool)\n",
      "[array([0., 1., 2., 3.], dtype=float32), array([False,  True,  True,  True])]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Int tensor\n",
    "int_tensor = tf.constant([0, 1, 2, 3], dtype=tf.int32)\n",
    "print(int_tensor)\n",
    "\n",
    "# Casting\n",
    "float_tensor = tf.cast(int_tensor, dtype=tf.float32)\n",
    "bool_tensor = tf.cast(int_tensor, dtype=tf.bool)\n",
    "print(int_tensor, float_tensor, bool_tensor)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run([float_tensor, bool_tensor]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.5)  `tf.Tensor` 객체의 속성\n",
    "1. **dtype**: 해당 Tensor내 요소들의 데이터 타입.\n",
    "2. **shape**: 해당 Tensor의 모양.\n",
    "3. device: 해당 Tensor가 생성되기로 한 장비의 이름.\n",
    "4. graph: 해당 Tensor가 속한 그래프.\n",
    "5. name: 해당 Tensor의 이름.\n",
    "6. op: 해당 Tensor를 출력하는 operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Variables\n",
    "\n",
    "TensorFlow에서 variable은 Program에 의해 관리되는 공유되고 보존되는 상태를 표현하는데 가장 좋은 방법이다.(원문참조)\n",
    "Variable들은 `tf.Variable` 클래스를 통해 다뤄진다. `tf.Variable`는 operation들을 해당 객체에 실행시켜 값을 변환할 수 있는 tensor를 나타낸다. `tf.Tensor` 객체와 달리, 하나의 `sessesion.run` call 밖에서 관리된다.\n",
    "\n",
    "Internally, a tf.Variable stores a persistent tensor. Specific ops allow you to read and modify the values of this tensor. These modifications are visible across multiple tf.Sessions, so multiple workers can see the same values for a tf.Variable.\n",
    "\n",
    "> 원문: A TensorFlow variable is the best way to represent shared, persistent state manipulated by your program.\n",
    "\n",
    "\n",
    "## B.1) Variable 생성\n",
    "\n",
    "variable을 만드는 가장좋은 방법은 `tf.get_variable`함수를 호출하는 것이다.  \n",
    "기본적으로, 해당함수는 정해진 모양(shape)의 값들을 `tf.float32`의 데이터 타입으로 랜덤하게 초기화 한다.  \n",
    "그리고 기본적인 값의 초기화 함수는 `tf.glorot_uniform_initializer`을 이용한다.\n",
    "\n",
    "기본값 이외에 옵션을 주어 설정이 가능하다.\n",
    "\n",
    "> Note: initializer로 `tf.Tensor`를 이용시, shape을 지정하면 안된다. Variable의 shape으론, 해당 tensor의 shape이 사용된다.\n",
    "\n",
    "The best way to create a variable is to call the tf.get_variable function. This function requires you to specify the Variable's name. This name will be used by other replicas to access the same variable, as well as to name this variable's value when checkpointing and exporting models. tf.get_variable also allows you to reuse a previously created variable of the same name, making it easy to define models which reuse layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'my_variable:0' shape=(1, 2, 3) dtype=float32_ref>\n",
      "<tf.Variable 'my_int_variable:0' shape=(1, 2, 3) dtype=int32_ref>\n",
      "<tf.Variable 'other_variable:0' shape=(2,) dtype=int32_ref>\n",
      "my_variable:  [[[-1.0450147   0.7702333  -0.83383346]\n",
      "  [-0.04738069 -0.7891766   0.8842331 ]]]\n",
      "my_variable:  [[[-1.0450147   0.7702333  -0.83383346]\n",
      "  [-0.04738069 -0.7891766   0.8842331 ]]]\n",
      "my_int_variable:  [[[0 0 0]\n",
      "  [0 0 0]]]\n",
      "other_variable:  [23 42]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Random values\n",
    "my_variable = tf.get_variable(\"my_variable\", [1, 2, 3])\n",
    "print(my_variable)\n",
    "\n",
    "# Zero values\n",
    "my_int_variable = tf.get_variable(\"my_int_variable\", [1, 2, 3], dtype=tf.int32,\n",
    "  initializer=tf.zeros_initializer)\n",
    "print(my_int_variable)\n",
    "\n",
    "# Constants\n",
    "other_variable = tf.get_variable(\"other_variable\", dtype=tf.int32,\n",
    "  initializer=tf.constant([23, 42]))\n",
    "print(other_variable)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('my_variable: ', sess.run(my_variable))\n",
    "    print('my_variable: ', sess.run(my_variable))\n",
    "    print('my_int_variable: ', sess.run(my_int_variable))\n",
    "    print('other_variable: ', sess.run(other_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2) Variable Collections\n",
    "\n",
    "기본적으로 모든 `tf.Variable`들은 아래의 두 collections에 존재한다.\n",
    "\n",
    "- `tf.GraphKeys.GLOBAL_VARIABLES`: 여러 기기들간에 공유될 수 있는 `variable`들\n",
    "- `tf.GraphKeys.TRAINABLE_VARIABLES`: TensorFlow가 gradient를 계산할 수 있는 `variable`들\n",
    "\n",
    "`variable`들이 학습(업데이트)되는것을 원치 않는다면, `tf.GraphKeys.LOCAL_VARIABLES`에 추가하여야 한다.  \n",
    "혹은 `variable`생성시 `trainable=False`로 하여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom collection:  [<tf.Variable 'my_local:0' shape=() dtype=float32_ref>]\n",
      "Local Variables:  [<tf.Variable 'my_local:0' shape=() dtype=float32_ref>]\n",
      "Global Variables:  [<tf.Variable 'my_non_trainable:0' shape=() dtype=float32_ref>, <tf.Variable 'my_trainable:0' shape=() dtype=float32_ref>]\n",
      "Trainable Variables:  [<tf.Variable 'my_local:0' shape=() dtype=float32_ref>, <tf.Variable 'my_trainable:0' shape=() dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "my_local = tf.get_variable(\"my_local\", shape=(),\n",
    "collections=[tf.GraphKeys.LOCAL_VARIABLES])\n",
    "\n",
    "my_non_trainable = tf.get_variable(\"my_non_trainable\",\n",
    "                                   shape=(),\n",
    "                                   trainable=False)\n",
    "\n",
    "my_trainable = tf.get_variable(\"my_trainable\",\n",
    "                                shape=(),\n",
    "                                trainable=True)\n",
    "\n",
    "tf.add_to_collection(\"my_collection_name\", my_local)\n",
    "\n",
    "print('Custom collection: ', tf.get_collection(\"my_collection_name\"))\n",
    "print('Local Variables: ', tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES))\n",
    "print('Global Variables: ', tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "print('Trainable Variables: ', tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3) Variable 초기화\n",
    "\n",
    "`variable`을 사용전에 **반드시 초기화** 시켜주어야 한다.\n",
    "한번에 **학습가능한 `variable`들**을 초기화 시키려면, `tf.global_variables_initializer()`를 사용하면 된다. 해당 함수는 `tf.GraphKeys.GLOBAL_VARIABLES` collection에 있는 `variable`들을 초기화 시켜주는 operation을 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.78687054, 0.2129265]\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value my_local\n\t [[{{node _retval_my_local_0_0}} = _Retval[T=DT_FLOAT, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](my_local)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value my_local\n\t [[{{node _retval_my_local_0_0}} = _Retval[T=DT_FLOAT, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](my_local)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6e484a4d5c0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_non_trainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_trainable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_local\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value my_local\n\t [[{{node _retval_my_local_0_0}} = _Retval[T=DT_FLOAT, index=0, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](my_local)]]"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run([my_non_trainable, my_trainable]))\n",
    "print(sess.run(my_local))  # ERROR!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3-a) 초기화 되지 않은 variable 확인하기\n",
    "\n",
    "`tf.report_uninitialized_variables()`함수는 초기화 되지 않은 `variable`들을 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'my_local'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.report_uninitialized_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.43277323"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(my_local.initializer)\n",
    "sess.run(my_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.report_uninitialized_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3-b) 초기화의 주의점\n",
    "\n",
    "`tf.global_variables_initializer()`는 variable들의 순서를 지정하여 초기화 하지 않는다. 때문에 초기화 값이 다른 variable의 값과 연관되어 있다면, 에러가 발생할 확률이 높다. 초기화 값은 `my_varible.initialized_value()`을 이용한다.\n",
    "\n",
    "> 참고. 간단한 테스트에서는 에러발생을 보이지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'var_1:0' shape=() dtype=float32_ref> <tf.Variable 'var_2:0' shape=() dtype=float32_ref>\n",
      "Var_1:  0.0\n",
      "Var_2:  1.0\n",
      "<tf.Variable 'var_1:0' shape=() dtype=float32_ref> <tf.Variable 'var_2:0' shape=() dtype=float32_ref>\n",
      "Var_1:  0.0\n",
      "Var_2:  1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Random values\n",
    "var_1 = tf.get_variable(\"var_1\", shape=(), initializer=tf.zeros_initializer())\n",
    "var_2 = tf.get_variable(\"var_2\", dtype=tf.float32, initializer=var_1 + 1.0)  # ERROR???\n",
    "print(var_1, var_2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Var_1: ', sess.run(var_1))\n",
    "    print('Var_2: ', sess.run(var_2))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "var_1 = tf.get_variable(\"var_1\", shape=(), initializer=tf.zeros_initializer())\n",
    "var_2 = tf.get_variable(\"var_2\", initializer=var_1.initialized_value() + 1)\n",
    "print(var_1, var_2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Var_1: ', sess.run(var_1))\n",
    "    print('Var_2: ', sess.run(var_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.4) Variable 사용\n",
    "\n",
    "TensorFlow 그래프에서 `tf.Variable`의 값을 사용하기 위해서는 `tf.Tensor`처럼 다루면 된다.  \n",
    "Variable에 값을 할당하려면 `tf.Variable` 인스턴스에서 assign, assign_add 같은 메소드를 사용하면된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'v:0' shape=() dtype=float32_ref>\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "[0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "v = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\n",
    "w = v + 1\n",
    "\n",
    "print(v)\n",
    "print(w)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run([v, w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[1.0, 1.0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "v = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\n",
    "assignment = v.assign_add(1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(v))\n",
    "    print(sess.run([v, assignment]))\n",
    "    print(sess.run(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[1.0, 1.0]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "v = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\n",
    "assignment = v.assign_add(1)\n",
    "with tf.control_dependencies([assignment]):\n",
    "    w = v.read_value()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(v))\n",
    "    print(sess.run([v, w]))\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.5) Sharing variables\n",
    "\n",
    "Variable을 공유하는 방법은 2가지가 존재한다.\n",
    "1. `tf.Variable` 객체를 직접 입력(Explicit)\n",
    "2. `tf.Varibale` 객체를 `tf.variable_scope` 객체 안에 wrapping(Implicit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드에서, 마지막 줄은 \"weights\", \"biases\" `variable`들이 이미 존재하기 때문에 실패한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-13-c0bc140e7c42>\", line 6, in conv_relu\n    initializer=tf.random_normal_initializer())\n  File \"<ipython-input-14-3c1f088754fd>\", line 3, in <module>\n    x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\n  File \"/home/seungbae/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3c1f088754fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# This fails.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-c0bc140e7c42>\u001b[0m in \u001b[0;36mconv_relu\u001b[0;34m(input, kernel_shape, bias_shape)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Create variable named \"weights\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     weights = tf.get_variable(\"weights\", kernel_shape,\n\u001b[0;32m----> 6\u001b[0;31m         initializer=tf.random_normal_initializer())\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Create variable named \"biases\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     biases = tf.get_variable(\"biases\", bias_shape,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1485\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1235\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    538\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    859\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 861\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    862\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-13-c0bc140e7c42>\", line 6, in conv_relu\n    initializer=tf.random_normal_initializer())\n  File \"<ipython-input-14-3c1f088754fd>\", line 3, in <module>\n    x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\n  File \"/home/seungbae/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.random_normal([1,10,10,32])\n",
    "input2 = tf.random_normal([1,20,20,32])\n",
    "x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\n",
    "y = conv_relu(x, kernel_shape=[5, 5, 32, 32], bias_shape = [32])  # This fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Variables:  [<tf.Variable 'weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>, <tf.Variable 'biases:0' shape=(32,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print('Global Variables: ', tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.variable_scope()`사용시 `variable`들의 이름이 scope아래에 생성되어 재사용이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable model/conv1/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-13-c0bc140e7c42>\", line 6, in conv_relu\n    initializer=tf.random_normal_initializer())\n  File \"<ipython-input-16-3e2611218a85>\", line 4, in my_image_filter\n    relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n  File \"<ipython-input-17-b801261acc40>\", line 2, in <module>\n    output1 = my_image_filter(input1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b801261acc40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_image_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_image_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-3e2611218a85>\u001b[0m in \u001b[0;36mmy_image_filter\u001b[0;34m(input_images)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# Variables created here will be named \"conv1/weights\", \"conv1/biases\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mrelu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conv2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Variables created here will be named \"conv2/weights\", \"conv2/biases\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c0bc140e7c42>\u001b[0m in \u001b[0;36mconv_relu\u001b[0;34m(input, kernel_shape, bias_shape)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Create variable named \"weights\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     weights = tf.get_variable(\"weights\", kernel_shape,\n\u001b[0;32m----> 6\u001b[0;31m         initializer=tf.random_normal_initializer())\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Create variable named \"biases\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     biases = tf.get_variable(\"biases\", bias_shape,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1485\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1235\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    538\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    859\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 861\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    862\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable model/conv1/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-13-c0bc140e7c42>\", line 6, in conv_relu\n    initializer=tf.random_normal_initializer())\n  File \"<ipython-input-16-3e2611218a85>\", line 4, in my_image_filter\n    relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n  File \"<ipython-input-17-b801261acc40>\", line 2, in <module>\n    output1 = my_image_filter(input1)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"model\"):\n",
    "    output1 = my_image_filter(input1)\n",
    "with tf.variable_scope(\"model\", reuse=False):\n",
    "    output2 = my_image_filter(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])\n",
    "\n",
    "input1 = tf.random_normal([1,10,10,32])\n",
    "input2 = tf.random_normal([1,20,20,32])\n",
    "    \n",
    "with tf.variable_scope(\"model\"):\n",
    "    output1 = my_image_filter(input1)\n",
    "with tf.variable_scope(\"model\", reuse=True):\n",
    "    output2 = my_image_filter(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Variables:  [<tf.Variable 'model/conv1/weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>, <tf.Variable 'model/conv1/biases:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'model/conv2/weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>, <tf.Variable 'model/conv2/biases:0' shape=(32,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print('Global Variables: ', tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "`Tensor`와 `Variable`은 TebsorFlow 프로그램 작성시 가장 흔하게 다루게 되는 객체이다.  \n",
    "- `Tensor`는 모델에 입력(`tf.data`)되거나 모델에 흐르는 데이터를 다룰때 자주 접근하게 되고,\n",
    "- `Variable`은 모델의 parameter(ex. LSTM의 state, global steps)에 접근하여 이를 수정하거나 관리할 때 다루게 된다.\n",
    "\n",
    "`Tensor`는 shape과 dtype에 신경을 자주 쓰게 되며, `Variable`은 collection별 초기화 및 값 할당을 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [TensorFlow Guide: Tensors](https://www.tensorflow.org/guide/tensors)\n",
    "2. [TensorFlow API: tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor)\n",
    "<!-- 3. [Persistent Data Structure](https://en.wikipedia.org/wiki/Persistent_data_structure) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
