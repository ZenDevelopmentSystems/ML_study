{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fall of RNN / LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 재발 신경 네트워크 (RNN), 장기 단기 기억 (LSTM) 및 모든 변종에 빠졌습니다. 이제 그들을 버려야 할 때입니다!\n",
    "\n",
    "2014 년입니다. LSTM과 RNN은 죽은 사람들로부터 큰 반향을 불러옵니다. 우리 모두 Colah의 블로그와 Karpathy의 RNN에 대한 독서를 읽었습니다. 그러나 우리는 모두 젊고 경험이 없었습니다. 몇 년 동안이 방법은 시퀀스 학습, 시퀀스 번역 (seq2seq)을 해결할 수있는 방법이었으며 음성으로 텍스트를 이해하고 Siri, Cortana, Google 음성 보조자 인 Alexa를 인상했습니다. 또한 문서를 다른 언어 나 신경 기계 번역으로 번역 할 수있는 기계 번역을 잊지 말고 이미지를 텍스트, 텍스트, 이미지 캡션 및 비디오 캡션으로 번역 할 수 있습니다.\n",
    "\n",
    "그 후 다음 해 (2015-16)에 ResNet and Attention이 나왔습니다. LSTM이 영리한 우회 기술이라는 것을 더 잘 이해할 수 있습니다. 또한 관심은 MLP 네트워크가 컨텍스트 벡터에 의해 영향을받는 평균 네트워크로 대체 될 수 있음을 보여주었습니다. 나중에 이것에 대한 자세한 내용.\n",
    "\n",
    "그것은 단지 2 년이 더 걸렸지 만 오늘날 우리는 확실히 말할 수 있습니다 :\n",
    "\n",
    "\"당신의 RNN과 LSTM을 버리십시오, 그들은 좋지 않습니다!\"\n",
    "그러나 그것에 대한 우리의 말을 듣지 마십시오. 또한주의를 기반으로 한 네트워크가 Google, Facebook, Salesforce에서 점점 더 많이 사용된다는 증거를 확인하십시오. 이러한 모든 회사는 RNN 및 관심 기반 모델의 변형을 대체했으며 이는 시작에 불과합니다. RNN은 관심 기반 모델보다 교육 및 실행에 더 많은 리소스가 필요하기 때문에 모든 애플리케이션에서 일 수를 계산합니다. 자세한 내용은이 게시물을 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But Why?\n",
    "\n",
    "이 화살표는 장기 정보가 현재 셀을 처리하기 전에 모든 셀을 순차적으로 이동해야한다는 것을 의미합니다. 이것은 작은 숫자 $<0$으로 많은 시간을 곱함으로써 쉽게 손상 될 수 있음을 의미합니다. 이것은 사라지는 그라디언트의 원인입니다.\n",
    "\n",
    "구조를 위해 LSTM 모듈을 보았습니다.이 모듈은 오늘날 다중 스위치 게이트로 간주 될 수 있습니다. ResNet과 같은 비트는 유닛을 우회하여 더 긴 시간 간격을 기억할 수 있습니다. 따라서 LSTM에는 사라지는 그라데이션 문제 중 일부를 제거 할 수있는 방법이 있습니다.\n",
    "\n",
    "그러나 위의 그림에서 볼 수 있듯이 모든 것이 아닙니다. 아직도 오래된 셀에서 현재 셀까지 순차적 인 경로가 있습니다. 실제로 경로는 더 복잡해졌습니다. 왜냐하면 첨가제가 있고 그것에 연결된 가지를 잊어 버리기 때문입니다. 의심의 여지가 LSTM과 GRU 및 파생 상품은 많은 장기 정보를 배울 수 있습니다! 결과보기 그러나 그들은 1000s 또는 10,000s가 아닌 100s 시퀀스를 기억할 수 있습니다.\n",
    "\n",
    "RNN의 한 가지 문제는 하드웨어 친화적이지 않다는 것입니다. 설명 드리죠. 네트워크를 빠르게 교육 할 필요가없는 많은 리소스가 필요합니다. 또한 클라우드에서 이러한 모델을 실행하는 데 많은 리소스가 필요하며 음성 텍스트에 대한 수요가 급격히 증가하고 있으므로 클라우드는 확장 할 수 없습니다. 우리는 아마존 에코에 바로 가장자리에서 처리해야합니다! 자세한 내용은 아래 참고를 참조하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you do?\n",
    "\n",
    "순차적 인 처리가 피해야하는 경우, 우리는 과거를 알고 미래에 영향을 미치고 싶어하는 실시간 인과 관계 데이터를 다루기 때문에 \"look-ahead\"또는 \"look-back\"하는 유닛을 찾을 수 있습니다 결정. 예를 들어 우리가 모든 데이터를 가지고 더 많은 시간을 할애 할 수있는 문장을 번역하거나 기록한 비디오를 분석하는 것은 아닙니다. 이러한 룩 - 백 / 어드밴스 단위는 신경 관련 모듈이며, 여기에서 이미 설명 하였다.\n",
    "\n",
    "구조에, 그리고 여러 신경주의 모듈을 결합, 아래 그림과 같은 \"계층 신경주의 인코더\"온다 :\n",
    "\n",
    "과거를 조사하는 더 좋은 방법은 관심 모듈을 사용하여 과거의 인코딩 된 모든 벡터를 컨텍스트 벡터 Ct로 요약하는 것입니다.\n",
    "\n",
    "여기 신경망의 계층 구조와 매우 유사한 관심 모듈의 계층 구조가 있음을 주목하십시오. 이것은 또한 아래의 주 3에보고 된 시간적 컨벌루션 네트워크 (TCN)와 유사합니다.\n",
    "\n",
    "계층 적 신경주의 인코더에서 다중 계층의주의는 최근의 작은 부분, 예를 들어 100 개의 벡터를 볼 수 있지만 위의 계층은 100 x 100 벡터의 정보를 효과적으로 통합하는 이러한주의 모듈 중 100 개를 볼 수 있습니다. 이것은 10,000 개의 과거 벡터에 대한 계층 적 신경주의 인코더의 능력을 확장합니다.\n",
    "\n",
    "> 이것은 과거를 더 되돌아보고 미래에 영향을 미칠 수있는 방법입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [The fall of RNN / LSTM](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
